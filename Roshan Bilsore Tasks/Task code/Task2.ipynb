{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de9a722-f8f8-49cd-a21a-11064eedae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing\n",
    "#task1 : Tokenisation using nltk and spaCy tokeniser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff50f15b-0b29-4c9d-9883-5d23f12369d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "204efefd-0270-4c20-9d71-93f0d6b3dd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 6.0 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 3.4 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 2.7 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 2.1/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 2.6/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 2.9/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 3.9/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------ --------------------- 5.8/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.6/12.8 MB 2.7 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.8/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 2.4 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.3 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 8.7/12.8 MB 2.4 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.9/12.8 MB 2.4 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 2.3 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 2.3 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.2/12.8 MB 2.3 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.5/12.8 MB 2.3 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.9 MB/s  0:00:06\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1df8c3c-b9a3-433f-aee0-9a3d612d2db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0161232a-53cd-41d4-9f6a-2a7bb4941147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: spacy in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.8.11)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (0.24.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.3.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: typer>=0.24.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (0.24.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dell\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=12.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3bb7af5-ba12-4a86-ad96-264d3e7f695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "314deb8c-9d65-46be-b640-bdce6b4e3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99eea40a-4f49-49f3-a54d-a99204cfb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12f33691-a755-450f-90de-1e6c73fbdb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dr. Smith bought 5 apples for $10.50. \n",
    "He said, \"That's cheap!\" and left.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d38dcd9b-f1bd-4476-92fa-6ab8b311e6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens:\n",
      "['Dr.', 'Smith', 'bought', '5', 'apples', 'for', '$', '10.50', '.', 'He', 'said', ',', '``', 'That', \"'s\", 'cheap', '!', \"''\", 'and', 'left', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk_tokens = word_tokenize(text)\n",
    "print(\"NLTK Tokens:\")\n",
    "print(nltk_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dfa126f-ca08-4a68-b4b7-3c22c397efb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Tokens:\n",
      "['Dr.', 'Smith', 'bought', '5', 'apples', 'for', '$', '10.50', '.', '\\n', 'He', 'said', ',', '\"', 'That', \"'s\", 'cheap', '!', '\"', 'and', 'left', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "\n",
    "print(\"spaCy Tokens:\")\n",
    "print(spacy_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b0a9dc5-f170-44ad-8093-f14171d40582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK token count: 21\n",
      "spaCy token count: 22\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK token count:\", len(nltk_tokens))\n",
    "print(\"spaCy token count:\", len(spacy_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b34353c-f6e7-4c39-ab2e-2cf24b1155f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task B. Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be921f9d-706e-48be-af31-b9f9e3296b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff3c2d6c-eeca-4f81-a155-3b0caf426680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85b21eff-ecd7-4230-ae72-dd10efd9f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (NLTK tokens):\n",
      "['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', '.']\n",
      "\n",
      "After Stopword Removal (NLTK):\n",
      "['Natural', 'language', 'processing', 'field', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Natural language processing is a field of artificial intelligence \n",
    "that focuses on the interaction between computers and humans.\"\"\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "filtered_nltk = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "\n",
    "print(\"Before (NLTK tokens):\")\n",
    "print(tokens)\n",
    "\n",
    "print(\"\\nAfter Stopword Removal (NLTK):\")\n",
    "print(filtered_nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "681e56a7-2589-46cd-ac12-527bbab6afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C. Lemmatization & Stemmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e00bd157-4cf3-478a-ab96-fceee9c1807d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba4dfe56-5839-48f0-88aa-9e7ba709a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "517933c4-0c84-4240-8a8c-c984847a4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The students were studying different studies and wrote reports.\n",
    "They were running, jumping, and playing happily.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d20d4e1-9b7e-4dd3-bdcd-83e92aebb545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'students',\n",
       " 'were',\n",
       " 'studying',\n",
       " 'different',\n",
       " 'studies',\n",
       " 'and',\n",
       " 'wrote',\n",
       " 'reports',\n",
       " 'they',\n",
       " 'were',\n",
       " 'running',\n",
       " 'jumping',\n",
       " 'and',\n",
       " 'playing',\n",
       " 'happily']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "\n",
    "# keep only words\n",
    "words = [w.lower() for w in tokens if w.isalpha()]\n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a20f96c-3d64-4c5a-981d-166d305e4a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming Output:\n",
      "['the', 'student', 'were', 'studi', 'differ', 'studi', 'and', 'wrote', 'report', 'they', 'were', 'run', 'jump', 'and', 'play', 'happili']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(w) for w in words]\n",
    "\n",
    "print(\"Stemming Output:\")\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8db513ba-6c66-448b-ab57-39257ae23305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Output:\n",
      "['the', 'student', 'were', 'studying', 'different', 'study', 'and', 'wrote', 'report', 'they', 'were', 'running', 'jumping', 'and', 'playing', 'happily']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "print(\"Lemmatization Output:\")\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5887cef-61bf-4f2b-8cb7-61fdbab614c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Stemmed</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>students</td>\n",
       "      <td>student</td>\n",
       "      <td>student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>were</td>\n",
       "      <td>were</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>studying</td>\n",
       "      <td>studi</td>\n",
       "      <td>studying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>different</td>\n",
       "      <td>differ</td>\n",
       "      <td>different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>studies</td>\n",
       "      <td>studi</td>\n",
       "      <td>study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wrote</td>\n",
       "      <td>wrote</td>\n",
       "      <td>wrote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reports</td>\n",
       "      <td>report</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>they</td>\n",
       "      <td>they</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>were</td>\n",
       "      <td>were</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>running</td>\n",
       "      <td>run</td>\n",
       "      <td>running</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jumping</td>\n",
       "      <td>jump</td>\n",
       "      <td>jumping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>playing</td>\n",
       "      <td>play</td>\n",
       "      <td>playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>happily</td>\n",
       "      <td>happili</td>\n",
       "      <td>happily</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Original  Stemmed Lemmatized\n",
       "0         the      the        the\n",
       "1    students  student    student\n",
       "2        were     were       were\n",
       "3    studying    studi   studying\n",
       "4   different   differ  different\n",
       "5     studies    studi      study\n",
       "6         and      and        and\n",
       "7       wrote    wrote      wrote\n",
       "8     reports   report     report\n",
       "9        they     they       they\n",
       "10       were     were       were\n",
       "11    running      run    running\n",
       "12    jumping     jump    jumping\n",
       "13        and      and        and\n",
       "14    playing     play    playing\n",
       "15    happily  happili    happily"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Original\": words,\n",
    "    \"Stemmed\": stems,\n",
    "    \"Lemmatized\": lemmas\n",
    "})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fc8f0ed-36bc-440a-a4b2-6f81a8fbafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D. Handling punctuation, special characters, emojis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b1ebb63-fcf1-4216-a0c1-823fb5fd947f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "   ---------------------------------------- 0.0/608.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 608.4/608.4 kB 10.7 MB/s  0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4018796-25ff-48fc-804e-5980dc139ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf5c0554-5bf8-4fd6-9bc7-a6365dc9e2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Wow!!! This NLP lab is awesome ðŸ˜„ðŸ˜„ \n",
      "Follow @openai and use #MachineLearning #AI!!!\n",
      "Contact me at test@mail.com\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Wow!!! This NLP lab is awesome ðŸ˜„ðŸ˜„ \n",
    "Follow @openai and use #MachineLearning #AI!!!\n",
    "Contact me at test@mail.com\"\"\"\n",
    "print(\"Original Text:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21880d7d-6c2b-4d4e-9219-a0e238113b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Emoji Removal: Wow!!! This NLP lab is awesome  \n",
      "Follow @openai and use #MachineLearning #AI!!!\n",
      "Contact me at test@mail.com\n"
     ]
    }
   ],
   "source": [
    "no_emoji = emoji.replace_emoji(text, replace='')\n",
    "print(\"\\nAfter Emoji Removal:\", no_emoji)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dd9553d-e9fc-4588-8654-08ce9a2db254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Removing @mentions & #hashtags:\n",
      " Wow!!! This NLP lab is awesome  \n",
      "Follow  and use  !!!\n",
      "Contact me at test.com\n"
     ]
    }
   ],
   "source": [
    "clean1 = re.sub(r'[@#]\\w+', '', no_emoji)\n",
    "print(\"\\nAfter Removing @mentions & #hashtags:\\n\", clean1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91c83a71-ec08-4434-9a52-78bd0366c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E- Lowecasing and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6996ebc6-f37c-49f1-9a39-adbec9fb3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"  My name   is    Roshan Bilsore i lerning nlp Lowercasing and   .\n",
    "It Has MIXED case LETTERS   and   irregular   spacing.   \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66a85fbe-9011-4434-b777-0a3e3fe4325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowercase Text:\n",
      "   my name   is    roshan bilsore i lerning nlp lowercasing and   .\n",
      "it has mixed case letters   and   irregular   spacing.   \n"
     ]
    }
   ],
   "source": [
    "lower_text = text.lower()\n",
    "print(\"\\nLowercase Text:\\n\", lower_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f480d2d-07c5-4fad-83f3-5d87ff4f15bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalized Text:\n",
      " my name is roshan bilsore i lerning nlp lowercasing and . it has mixed case letters and irregular spacing.\n"
     ]
    }
   ],
   "source": [
    "normalized_text = re.sub(r'\\s+', ' ', lower_text).strip()\n",
    "print(\"\\nNormalized Text:\\n\", normalized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49522c59-5200-48ed-ae82-34a4e63d5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task - E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "695b33c7-d323-4317-bdcc-aba29d666954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ORIGINAL TEXT =====\n",
      "\n",
      "Hello, contact us at support@test.com or admin123@company.org.\n",
      "My backup email is user_007@mail.co.in.\n",
      "I have 2 laptops and 3 phones. Total cost is 45000 rupees.\n",
      "This    text   has     irregular spacing.\n",
      "\n",
      "\n",
      "===== EXTRACTED EMAILS =====\n",
      "support@test.com\n",
      "admin123@company.org\n",
      "user_007@mail.co.in\n",
      "\n",
      "===== TEXT WITHOUT NUMBERS =====\n",
      "\n",
      "Hello, contact us at support@test.com or admin@company.org.\n",
      "My backup email is user_@mail.co.in.\n",
      "I have  laptops and  phones. Total cost is  rupees.\n",
      "This    text   has     irregular spacing.\n",
      "\n",
      "\n",
      "===== NORMALIZED TEXT =====\n",
      "Hello, contact us at support@test.com or admin@company.org. My backup email is user_@mail.co.in. I have laptops and phones. Total cost is rupees. This text has irregular spacing.\n",
      "\n",
      "===== FINAL CLEAN TEXT =====\n",
      "Hello, contact us at support@test.com or admin@company.org. My backup email is user_@mail.co.in. I have laptops and phones. Total cost is rupees. This text has irregular spacing.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"\n",
    "Hello, contact us at support@test.com or admin123@company.org.\n",
    "My backup email is user_007@mail.co.in.\n",
    "I have 2 laptops and 3 phones. Total cost is 45000 rupees.\n",
    "This    text   has     irregular spacing.\n",
    "\"\"\"\n",
    "\n",
    "print(\"===== ORIGINAL TEXT =====\")\n",
    "print(text)\n",
    "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'\n",
    "emails = re.findall(email_pattern, text)\n",
    "print(\"\\n===== EXTRACTED EMAILS =====\")\n",
    "for e in emails:\n",
    "    print(e)\n",
    "no_numbers = re.sub(r'\\d+', '', text)\n",
    "print(\"\\n===== TEXT WITHOUT NUMBERS =====\")\n",
    "print(no_numbers)\n",
    "normalized = re.sub(r'\\s+', ' ', no_numbers).strip()\n",
    "\n",
    "print(\"\\n===== NORMALIZED TEXT =====\")\n",
    "print(normalized)\n",
    "print(\"\\n===== FINAL CLEAN TEXT =====\")\n",
    "print(normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d78c7-53b3-49c2-acca-7e3aa4472fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
