{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1be07829",
   "metadata": {},
   "source": [
    "**2. Word Embedding Explorer**\n",
    "\n",
    "• Load pretrained GloVe embeddings.\n",
    "\n",
    "• Perform word similarity & analogy tasks (king - man + woman ≈ queen).\n",
    "\n",
    "• Perform t-SNE / PCA visualization of embeddings in 2D clusters.\n",
    "\n",
    "• Compare Word2Vec vs FastText vs GloVe.\n",
    "\n",
    "• Show how FastText handles out-of-vocabulary words better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load a pre-trained Word2Vec model (this may take a moment)\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Now you can access word vectors\n",
    "try:\n",
    "    vector = model[\"computer\"]\n",
    "    print(\"Vector found for 'computer'\")\n",
    "except KeyError:\n",
    "    print(\"Word not in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311421a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load models\n",
    "glove_model = api.load('glove-wiki-gigaword-100')\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "# Word similarity\n",
    "word = 'computer'\n",
    "print(f\"GloVe similar to '{word}':\", glove_model.most_similar(word, topn=5))\n",
    "print(f\"Word2Vec similar to '{word}':\", word2vec_model.most_similar(word, topn=5))\n",
    "print(f\"FastText similar to '{word}':\", fasttext_model.most_similar(word, topn=5))\n",
    "\n",
    "# Analogy task\n",
    "analogy = glove_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(\"GloVe: king - man + woman ≈\", analogy[0][0])\n",
    "analogy_w2v = word2vec_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(\"Word2Vec: king - man + woman ≈\", analogy_w2v[0][0])\n",
    "analogy_ft = fasttext_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "print(\"FastText: king - man + woman ≈\", analogy_ft[0][0])\n",
    "\n",
    "# OOV handling\n",
    "oov_word = 'unbelievablenes'  # Made-up word, likely OOV for all models loaded as KeyedVectors\n",
    "\n",
    "try:\n",
    "    print(\"GloVe OOV similarity:\", glove_model.most_similar(oov_word))\n",
    "except KeyError:\n",
    "    print(\"GloVe: OOV error\")\n",
    "try:\n",
    "    print(\"Word2Vec OOV similarity:\", word2vec_model.most_similar(oov_word))\n",
    "except KeyError:\n",
    "    print(\"Word2Vec: OOV error\")\n",
    "try:\n",
    "    # While FastText typically handles OOV words via subword embeddings,\n",
    "    # the 'most_similar' method on KeyedVectors from gensim.downloader\n",
    "    # will still raise a KeyError if the word is not in its primary vocabulary.\n",
    "    # To leverage FastText's true OOV capability, one would typically need\n",
    "    # to access the full FastText model and its 'get_vector' method.\n",
    "    print(\"FastText OOV similarity:\", fasttext_model.most_similar(oov_word, topn=5))\n",
    "except KeyError:\n",
    "    print(\"FastText: OOV error (KeyedVectors does not handle truly OOV words in most_similar)\")\n",
    "\n",
    "# Visualization (t-SNE on sample words)\n",
    "words = ['king', 'queen', 'man', 'woman', 'computer', 'phone', 'apple', 'banana']\n",
    "embeddings = np.array([glove_model[w] for w in words])\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
    "plt.title('t-SNE Visualization of GloVe Embeddings')\n",
    "plt.show()\n",
    "\n",
    "# PCA alternative\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embeddings_pca[:, 0], embeddings_pca[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, (embeddings_pca[i, 0], embeddings_pca[i, 1]))\n",
    "plt.title('PCA Visualization of GloVe Embeddings')\n",
    "plt.show()\n",
    "\n",
    "# Comparison: FastText handles OOV better, Word2Vec often more accurate for analogies, GloVe balances context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
