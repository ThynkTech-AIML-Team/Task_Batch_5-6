{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aefab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04cdbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Natural Language Processing is powerful! It helps computers understand human language.Tokenization is the first step in NLP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e12ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sentences = nltk.sent_tokenize(text)\n",
    "nltk_words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b46f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NLTK Tokenization ---\n",
      "Sentences: ['\\nNatural Language Processing is powerful!', 'It helps computers understand human language.Tokenization is the first step in NLP.']\n",
      "Words: ['Natural', 'Language', 'Processing', 'is', 'powerful', '!', 'It', 'helps', 'computers', 'understand', 'human', 'language.Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\confection\\__init__.py:38: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1 import BaseModel, Extra, ValidationError, create_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spaCy not working.\n",
      "Error: unable to infer type for attribute \"REGEX\"\n",
      "\n",
      "Program Completed Successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- NLTK Tokenization ---\")\n",
    "print(\"Sentences:\", nltk_sentences)\n",
    "print(\"Words:\", nltk_words)\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    spacy_tokens = [token.text for token in doc]\n",
    "\n",
    "    print(\"\\n--- spaCy Tokenization ---\")\n",
    "    print(\"Tokens:\", spacy_tokens)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nspaCy not working.\")\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "print(\"\\nProgram Completed Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade17d6",
   "metadata": {},
   "source": [
    "TASK 3 B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b03bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdd0364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing is powerful! It helps computers understand human language.Tokenization is the first step in NLP\"\n",
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ac5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [word for word in tokens if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24b31a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['Natural', 'Language', 'Processing', 'is', 'powerful', '!', 'It', 'helps', 'computers', 'understand', 'human', 'language.Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP']\n",
      "After: ['Natural', 'Language', 'Processing', 'powerful', '!', 'helps', 'computers', 'understand', 'human', 'language.Tokenization', 'first', 'step', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", tokens)\n",
    "print(\"After:\", filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435c9c1",
   "metadata": {},
   "source": [
    "TASK 3 C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b75fe4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2337ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15ab2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59eb73f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"studies\", \"studying\", \"study\",\n",
    "    \"running\", \"runs\", \"ran\",\n",
    "    \"better\", \"best\",\n",
    "    \"flying\", \"flies\",\n",
    "    \"happiness\", \"happily\",\n",
    "    \"children\", \"mice\",\n",
    "    \"playing\", \"played\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbb292a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original     Stemmed      Lemmatized  \n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"{:<12} {:<12} {:<12}\".format(\"Original\", \"Stemmed\", \"Lemmatized\"))\n",
    "print(\"-\" * 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dbb2747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies      studi        study       \n",
      "studying     studi        studying    \n",
      "study        studi        study       \n",
      "running      run          running     \n",
      "runs         run          run         \n",
      "ran          ran          ran         \n",
      "better       better       better      \n",
      "best         best         best        \n",
      "flying       fli          flying      \n",
      "flies        fli          fly         \n",
      "happiness    happi        happiness   \n",
      "happily      happili      happily     \n",
      "children     children     child       \n",
      "mice         mice         mouse       \n",
      "playing      play         playing     \n",
      "played       play         played      \n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    print(\"{:<12} {:<12} {:<12}\".format(word, stemmed, lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17deebee",
   "metadata": {},
   "source": [
    "TASK 3 D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4751cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fdd0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello My Friend!!! This is amazing ðŸ˜ðŸ˜ #NLP #AI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5dad088",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'@\\w+', '', text)\n",
    "text = re.sub(r'#\\w+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c416cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'[^\\w\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6b5232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = emoji.replace_emoji(text, replace='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61083be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: Hello My Friend This is amazing   \n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned text:\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6221c986",
   "metadata": {},
   "source": [
    "TASK 3 E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d81f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP is FUN and USING NLP we can MAKE text PREPROCESSING easier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43541d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41f48cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ece4ab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp is fun and using nlp we can make text preprocessing easier\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d0881",
   "metadata": {},
   "source": [
    "TASK 3 F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7aefdc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebb21f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "Contact us at company@gmail.com  or  info@yahoo.com.\n",
    "This text has  numbers 12345 and    extra   spaces.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a1e8e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails: ['company@gmail.com', 'info@yahoo.com.']\n"
     ]
    }
   ],
   "source": [
    "emails = re.findall(r'\\S+@\\S+', paragraph)\n",
    "print(\"Emails:\", emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55d0707e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without numbers: \n",
      "Contact us at company@gmail.com  or  info@yahoo.com.\n",
      "This text has  numbers  and    extra   spaces.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_numbers = re.sub(r'\\d+', '', paragraph)\n",
    "print(\"Without numbers:\", no_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e56ffa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized spaces:  Contact us at company@gmail.com or info@yahoo.com. This text has numbers 12345 and extra spaces. \n"
     ]
    }
   ],
   "source": [
    "clean_spaces = re.sub(r'\\s+', ' ', paragraph)\n",
    "print(\"Normalized spaces:\", clean_spaces)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
