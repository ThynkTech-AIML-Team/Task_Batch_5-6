{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b063d458-42cb-46f0-8fe6-9dccf33bfb6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2088738567.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install nltk spacy emoji\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install nltk spacy emoji\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e559d066-f47b-435a-9f8f-beebfda2a634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419f48a9-2feb-42dc-83ee-c2354f00ab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9626306-ddec-490b-a02d-a67cded22126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: emoji in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: click in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015cb146-3320-4c53-a59b-bc845e779edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (26.0.1)\n",
      "Requirement already satisfied: nltk in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: emoji in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: click in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in .\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install nltk emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "636c2e7a-af33-4dfb-896f-12fd8963e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK: 3.9.2\n",
      "emoji: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import nltk, emoji\n",
    "print(\"NLTK:\", nltk.__version__)\n",
    "print(\"emoji:\", emoji.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26591a3c-657b-4f40-aa52-29f5a6c8949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f084846d-0527-461d-84d9-bc98f0c421e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK word_tokenize:\n",
      "['Hello', '!', 'I', \"'m\", 'Amisha', 'üòä', '.', 'Email', ':', 'test.user', '@', 'company.com', '#', 'NLP', '@', 'openai', '!', '!', '!', 'Score', ':', '10/10', '.']\n",
      "\n",
      "Regex tokenizer (words only):\n",
      "['Hello', 'I', 'm', 'Amisha', 'Email', 'test', 'user', 'company', 'com', 'NLP', 'openai', 'Score', '10', '10']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "text = \"Hello! I'm Amisha üòä. Email: test.user@company.com #NLP @openai!!! Score: 10/10.\"\n",
    "\n",
    "nltk_tokens = word_tokenize(text)\n",
    "regex_tokens = RegexpTokenizer(r\"\\w+\").tokenize(text)\n",
    "\n",
    "print(\"NLTK word_tokenize:\")\n",
    "print(nltk_tokens)\n",
    "\n",
    "print(\"\\nRegex tokenizer (words only):\")\n",
    "print(regex_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f8e9f2e-ddad-4320-b33f-f753abfa1685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['This', 'is', 'a', 'simple', 'sentence', 'showing', 'how', 'stopwords', 'are', 'removed', 'from', 'the', 'text', '.']\n",
      "After : ['simple', 'sentence', 'showing', 'stopwords', 'removed', 'text']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "sample = \"This is a simple sentence showing how stopwords are removed from the text.\"\n",
    "tokens = word_tokenize(sample)\n",
    "after = [w for w in tokens if w.lower() not in stop_words and w.isalnum()]\n",
    "\n",
    "print(\"Before:\", tokens)\n",
    "print(\"After :\", after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "024a5087-442d-4caf-8815-bcfbe9efbe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies -> stem: studi | lemma(noun): study | lemma(verb): study\n",
      "studying -> stem: studi | lemma(noun): studying | lemma(verb): study\n",
      "study -> stem: studi | lemma(noun): study | lemma(verb): study\n",
      "cars -> stem: car | lemma(noun): car | lemma(verb): cars\n",
      "running -> stem: run | lemma(noun): running | lemma(verb): run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"studies\", \"studying\", \"study\", \"cars\", \"running\"]\n",
    "\n",
    "for w in words:\n",
    "    print(\n",
    "        w,\n",
    "        \"-> stem:\", stemmer.stem(w),\n",
    "        \"| lemma(noun):\", lemmatizer.lemmatize(w),\n",
    "        \"| lemma(verb):\", lemmatizer.lemmatize(w, pos=\"v\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72a2e64-5e90-4ee6-9083-8c30b02d6a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Wow!!! This is #Amazing üòçüòç Visit @Amisha now!!! Price is 999!!!\n",
      "After : Wow This is Amazing :smiling_face_with_heart eyes::smiling_face_with_heart eyes: Visit now Price is 999\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import re\n",
    "\n",
    "messy = \"Wow!!! This is #Amazing üòçüòç Visit @Amisha now!!! Price is 999!!!\"\n",
    "\n",
    "def clean_text(text):\n",
    "    text = emoji.demojize(text)           # convert emojis to text labels\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)      # remove @mentions\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text) # keep hashtag word without #\n",
    "    text = re.sub(r\"[!?\\.]{2,}\", \" \", text)  # remove repeated punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s:_]\", \" \", text)  # remove special chars\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()       # normalize spaces\n",
    "    return text\n",
    "\n",
    "print(\"Before:\", messy)\n",
    "print(\"After :\", clean_text(messy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "128cdd28-28f6-4b0f-938c-2567bf5cd311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: '   ThIs   Is  MIXED   Case   Text.   \\nNew   line   here.   '\n",
      "After : 'this is mixed case text. new line here.'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "mixed = \"   ThIs   Is  MIXED   Case   Text.   \\nNew   line   here.   \"\n",
    "lowered = mixed.lower()\n",
    "normalized = re.sub(r\"\\s+\", \" \", lowered).strip()\n",
    "\n",
    "print(\"Before:\", repr(mixed))\n",
    "print(\"After :\", repr(normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a1c960d-48d2-40ae-81d9-042027937437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['support@company.com', 'hr.team123@domain.co.in', 'first.last@uni.edu']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "para = \"Contact support@company.com, hr.team123@domain.co.in and first.last@uni.edu.\"\n",
    "email_pattern = r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\"\n",
    "\n",
    "print(re.findall(email_pattern, para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60ed678d-0e4c-4e38-9dfc-168788771171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: I have 2 laptops and 500 rupees.\n",
      "After : I have laptops and rupees.\n"
     ]
    }
   ],
   "source": [
    "text_num = \"I have 2 laptops and 500 rupees.\"\n",
    "no_numbers = re.sub(r\"\\d+\", \"\", text_num)\n",
    "no_numbers = re.sub(r\"\\s+\", \" \", no_numbers).strip()\n",
    "\n",
    "print(\"Before:\", text_num)\n",
    "print(\"After :\", no_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e388f491-3fc6-40c8-b409-659453000fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This has too many spaces.\n"
     ]
    }
   ],
   "source": [
    "text_spaces = \"This    has   too      many   spaces.\"\n",
    "print(re.sub(r\"\\s+\", \" \", text_spaces).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e543ad2-0b06-4bec-8c80-a20a70221c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
