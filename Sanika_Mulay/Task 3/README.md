BASIC TEXT CLASSIFICATION PROJECT



Objective:

To build a simple text classification model and compare 2–3 machine learning algorithms.



Dataset Used:

IMDb Sentiment Dataset



Movie reviews dataset



Two classes: Positive and Negative



Used for sentiment analysis



Steps Performed:



Text Cleaning:



Converted text to lowercase



Removed punctuation



Removed special characters



Removed extra spaces



Stopword Removal:



Removed common words such as “the”, “is”, “and”, etc.



Used NLTK stopwords



Text Vectorization:



Used CountVectorizer



Used TF-IDF Vectorizer



Converted text into numerical feature vectors



Models Used:



Naive Bayes



Simple and fast



Works well for text classification



Logistic Regression



Strong baseline model



Performs well with TF-IDF features



Support Vector Machine (Optional)



Effective for high-dimensional text data



Often gives high accuracy



Evaluation Metrics:



Accuracy



Measures overall correct predictions



Confusion Matrix



Shows correct and incorrect predictions for each class



Classification Report



Includes Precision, Recall, and F1-score



Bonus Tasks:



Compared CountVectorizer vs TF-IDF performance



Displayed top important words for Positive and Negative classes using model coefficients



Conclusion:

Logistic Regression with TF-IDF generally gave the best performance. TF-IDF performed better than CountVectorizer because it reduces the importance of frequently occurring words.



WORD EMBEDDING MINI TASK



Objective:

To understand and work with pretrained word embeddings.



Tasks Performed:



Loaded pretrained Word2Vec and/or GloVe embeddings



Similar Words:

Example:

king → queen, prince, royal



Analogy Task:

king - man + woman ≈ queen



PCA Visualization:



Selected 10–20 words



Reduced dimensions using PCA



Plotted words in 2D space



Bonus:



Compared similarity results between Word2Vec and GloVe



Observed slight differences in semantic relationships



Conclusion:

Word embeddings capture semantic meaning of words and allow similarity and analogy operations.



MINI NLP APPLICATION – FAKE NEWS DETECTION SYSTEM



Objective:

To build a machine learning model to classify news as Fake or Real.



Dataset:

Fake and Real News dataset

Two classes:



Fake



Real



Steps Performed:



Text Cleaning



Stopword Removal



TF-IDF Vectorization



Model Used:



Logistic Regression

OR



Naive Bayes



Evaluation:



Accuracy Score



Confusion Matrix



Classification Report



Top Important Words:



Extracted most important words for Fake and Real classes using model coefficients

BASIC TEXT CLASSIFICATION PROJECT



Objective:

To build a simple text classification model and compare 2–3 machine learning algorithms.



Dataset Used:

IMDb Sentiment Dataset



Movie reviews dataset



Two classes: Positive and Negative



Used for sentiment analysis



Steps Performed:



Text Cleaning:



Converted text to lowercase



Removed punctuation



Removed special characters



Removed extra spaces



Stopword Removal:



Removed common words such as “the”, “is”, “and”, etc.



Used NLTK stopwords



Text Vectorization:



Used CountVectorizer



Used TF-IDF Vectorizer



Converted text into numerical feature vectors



Models Used:



Naive Bayes



Simple and fast



Works well for text classification



Logistic Regression



Strong baseline model



Performs well with TF-IDF features



Support Vector Machine (Optional)



Effective for high-dimensional text data



Often gives high accuracy



Evaluation Metrics:



Accuracy



Measures overall correct predictions



Confusion Matrix



Shows correct and incorrect predictions for each class



Classification Report



Includes Precision, Recall, and F1-score



Bonus Tasks:



Compared CountVectorizer vs TF-IDF performance



Displayed top important words for Positive and Negative classes using model coefficients



Conclusion:

Logistic Regression with TF-IDF generally gave the best performance. TF-IDF performed better than CountVectorizer because it reduces the importance of frequently occurring words.



WORD EMBEDDING MINI TASK



Objective:

To understand and work with pretrained word embeddings.



Tasks Performed:



Loaded pretrained Word2Vec and/or GloVe embeddings



Similar Words:

Example:

king → queen, prince, royal



Analogy Task:

king - man + woman ≈ queen



PCA Visualization:



Selected 10–20 words



Reduced dimensions using PCA



Plotted words in 2D space



Bonus:



Compared similarity results between Word2Vec and GloVe



Observed slight differences in semantic relationships



Conclusion:

Word embeddings capture semantic meaning of words and allow similarity and analogy operations.



MINI NLP APPLICATION – FAKE NEWS DETECTION SYSTEM



Objective:

To build a machine learning model to classify news as Fake or Real.



Dataset:

Fake and Real News dataset

Two classes:



Fake



Real



Steps Performed:



Text Cleaning



Stopword Removal



TF-IDF Vectorization



Model Used:



Logistic Regression

OR



Naive Bayes



Evaluation:



Accuracy Score



Confusion Matrix



Classification Report



Top Important Words:



Extracted most important words for Fake and Real classes using model coefficients





