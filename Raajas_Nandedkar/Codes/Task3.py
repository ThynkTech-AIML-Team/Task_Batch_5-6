# =========================================
# TASK 3 â€” TEXT PREPROCESSING
# A â€” Tokenization
# B â€” Stopwords Removal
# C â€” Stemming vs Lemmatization
# D â€” Cleaning special chars/emojis
# E â€” Lowercase & normalization
# F â€” Regex tasks
# =========================================
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.tokenize import word_tokenize, wordpunct_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import re
import emoji


# -----------------------------
# PART A â€” TOKENIZATION
# -----------------------------
print("\n===== TASK 3A â€” TOKENIZATION =====")

text = "I can't believe NLP-based systems work!"

print("word_tokenize:", word_tokenize(text))
print("wordpunct_tokenize:", wordpunct_tokenize(text))


# -----------------------------
# PART B â€” STOPWORDS REMOVAL
# -----------------------------
print("\n===== TASK 3B â€” STOPWORDS REMOVAL =====")

stop_words = set(stopwords.words("english"))
tokens = word_tokenize("This is a simple example showing stopword removal")

filtered = [w for w in tokens if w.lower() not in stop_words]

print("Before:", tokens)
print("After:", filtered)


# -----------------------------
# PART C â€” STEMMING vs LEMMA
# -----------------------------
print("\n===== TASK 3C â€” STEMMING vs LEMMATIZATION =====")

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

words = word_tokenize("studies studying studied study")

for w in words:
    print(w,
          "Stem:", stemmer.stem(w),
          "Lemma:", lemmatizer.lemmatize(w))


# -----------------------------
# PART D â€” CLEAN TEXT
# -----------------------------
print("\n===== TASK 3D â€” CLEAN SPECIALS =====")

dirty = "Hello!!! ðŸ˜Š Check #NLP @user now!!!"

clean = emoji.replace_emoji(dirty, "")
clean = re.sub(r'[@#]\w+', '', clean)
clean = re.sub(r'[^\w\s]', '', clean)

print("Cleaned:", clean)


# -----------------------------
# PART E â€” LOWERCASE + NORMALIZE
# -----------------------------
print("\n===== TASK 3E â€” NORMALIZATION =====")

mixed = "   NLP   Is   VERY   COOL   "
normalized = " ".join(mixed.lower().split())

print("Normalized:", normalized)


# -----------------------------
# PART F â€” REGEX TASKS
# -----------------------------
print("\n===== TASK 3F â€” REGEX =====")

sample = """
Contact: help@test.com and admin@mail.org
Numbers 12345 present
Extra    spaces here
"""

emails = re.findall(r'\S+@\S+', sample)
print("Emails:", emails)

print("No numbers:", re.sub(r'\d+', '', sample))
print("Single spaces:", re.sub(r'\s+', ' ', sample))