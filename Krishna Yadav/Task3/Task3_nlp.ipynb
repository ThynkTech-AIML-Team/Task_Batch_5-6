{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b002cfd2-5ed2-465a-a3c9-770ac809d812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: nltk in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: gensim in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: matplotlib in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from nltk) (4.67.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from matplotlib) (12.1.1)\n",
      "Requirement already satisfied: pyparsing>=3 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: six>=1.5 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: wrapt in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Requirement already satisfied: colorama in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn nltk gensim matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5026c760-06a4-4816-bd5b-6176af78e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26a67e5-3c34-43cd-9a25-621a34bf4d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6423ad1-d72f-4b21-adfd-69dc3e55a24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: kagglesdk<1.0,>=0.1.14 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from kagglehub) (0.1.15)\n",
      "Requirement already satisfied: packaging in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from kagglehub) (6.0.3)\n",
      "Requirement already satisfied: requests in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from kagglehub) (2.32.5)\n",
      "Requirement already satisfied: tqdm in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from kagglehub) (4.67.3)\n",
      "Requirement already satisfied: protobuf in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from kagglesdk<1.0,>=0.1.14->kagglehub) (6.33.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from requests->kagglehub) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from requests->kagglehub) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from requests->kagglehub) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from requests->kagglehub) (2026.1.4)\n",
      "Requirement already satisfied: colorama in C:\\Users\\Admn\\anaconda3\\envs\\nlp_env\\Lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Dataset downloaded to: C:\\Users\\Admn\\.cache\\kagglehub\\datasets\\uciml\\sms-spam-collection-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# download SMS Spam dataset\n",
    "path = kagglehub.dataset_download(\"uciml/sms-spam-collection-dataset\")\n",
    "\n",
    "print(\"Dataset downloaded to:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b114d59-c5fc-4496-a6fd-62f038a7ed39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spam.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# show files inside downloaded dataset folder\n",
    "dataset_path = r\"C:\\Users\\Admn\\.cache\\kagglehub\\datasets\\uciml\\sms-spam-collection-dataset\\versions\\1\"\n",
    "\n",
    "os.listdir(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb75afa5-ba33-48f4-b36c-07b774a3e15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "file_path = r\"C:\\Users\\Admn\\.cache\\kagglehub\\datasets\\uciml\\sms-spam-collection-dataset\\versions\\1\\spam.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, encoding='latin-1')\n",
    "\n",
    "# keep only needed columns\n",
    "df = df[['v1','v2']]\n",
    "df.columns = ['label','text']\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24009bf7-7489-411e-8ad7-77285cf56415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4        nah dont think goes usf lives around though  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    return \" \".join(words)\n",
    "\n",
    "# apply cleaning\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f8a141-5f9c-4f31-b67f-689ee45ab49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 4457\n",
      "Testing samples: 1115\n"
     ]
    }
   ],
   "source": [
    "# features and labels\n",
    "X = df['clean_text']\n",
    "y = df['label']\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(X_train))\n",
    "print(\"Testing samples:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa50a927-847c-40dc-a508-5f10e2f0811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9802690582959641\n",
      "\n",
      "Confusion Matrix:\n",
      " [[961   4]\n",
      " [ 18 132]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       965\n",
      "        spam       0.97      0.88      0.92       150\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert text into numbers using CountVectorizer\n",
    "count_vec = CountVectorizer()\n",
    "\n",
    "X_train_count = count_vec.fit_transform(X_train)\n",
    "X_test_count = count_vec.transform(X_test)\n",
    "\n",
    "# train Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_count, y_train)\n",
    "\n",
    "# predictions\n",
    "pred_nb = nb_model.predict(X_test_count)\n",
    "\n",
    "# evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_nb))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, pred_nb))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9de22c95-6290-4578-94d9-54379ac9da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9515695067264573\n",
      "\n",
      "Confusion Matrix:\n",
      " [[961   4]\n",
      " [ 50 100]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.97       965\n",
      "        spam       0.96      0.67      0.79       150\n",
      "\n",
      "    accuracy                           0.95      1115\n",
      "   macro avg       0.96      0.83      0.88      1115\n",
      "weighted avg       0.95      0.95      0.95      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Logistic Regression Model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# predictions\n",
    "pred_lr = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "# evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred_lr))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, pred_lr))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f39d71b0-76e2-4f72-b17a-2d3308dc3b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Spam Words:\n",
      "\n",
      "         word  importance\n",
      "7488      txt    4.612192\n",
      "1909    claim    3.926982\n",
      "3113     free    3.795109\n",
      "6856     stop    3.778675\n",
      "4789   mobile    3.701939\n",
      "1661     call    3.213904\n",
      "6039    reply    3.122367\n",
      "5734    prize    2.956106\n",
      "7160     text    2.917993\n",
      "6364  service    2.605520\n",
      "\n",
      "Top Ham Words:\n",
      "\n",
      "       word  importance\n",
      "3795     im   -2.025610\n",
      "4478   ltgt   -2.021118\n",
      "3792    ill   -1.908342\n",
      "5207     ok   -1.899597\n",
      "6519    sir   -1.504458\n",
      "1986   come   -1.474839\n",
      "4247  later   -1.434345\n",
      "2217     da   -1.316825\n",
      "3345    got   -1.285468\n",
      "3310  going   -1.282684\n"
     ]
    }
   ],
   "source": [
    "# get feature names\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# get model coefficients\n",
    "coefficients = lr_model.coef_[0]\n",
    "\n",
    "# create dataframe of words and importance\n",
    "word_importance = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'importance': coefficients\n",
    "})\n",
    "\n",
    "# top spam words (positive values)\n",
    "top_spam = word_importance.sort_values(by='importance', ascending=False).head(10)\n",
    "\n",
    "# top ham words (negative values)\n",
    "top_ham = word_importance.sort_values(by='importance').head(10)\n",
    "\n",
    "print(\"Top Spam Words:\\n\")\n",
    "print(top_spam)\n",
    "\n",
    "print(\"\\nTop Ham Words:\\n\")\n",
    "print(top_ham)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5a8025a-12cc-4357-86a2-0795bf23bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# load pretrained Word2Vec model\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd43c66-ca2e-4285-a7fa-4698e2558eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# lightweight pretrained embedding model (fast)\n",
    "w2v_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "print(\"Model loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3bba32-d8f5-49f2-b452-75e964383c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.8236179351806641),\n",
       " ('queen', 0.7839043140411377),\n",
       " ('ii', 0.7746230363845825),\n",
       " ('emperor', 0.7736247777938843),\n",
       " ('son', 0.766719400882721),\n",
       " ('uncle', 0.7627150416374207),\n",
       " ('kingdom', 0.7542160749435425),\n",
       " ('throne', 0.7539914846420288),\n",
       " ('brother', 0.7492412328720093),\n",
       " ('ruler', 0.7434253096580505)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar words to \"king\"\n",
    "w2v_model.most_similar(\"king\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "456436fd-cc0c-4bd2-9bf6-2e68ad0d78fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8523604869842529),\n",
       " ('throne', 0.7664334177970886),\n",
       " ('prince', 0.759214460849762),\n",
       " ('daughter', 0.7473882436752319),\n",
       " ('elizabeth', 0.7460220456123352),\n",
       " ('princess', 0.7424570322036743),\n",
       " ('kingdom', 0.7337412238121033),\n",
       " ('monarch', 0.721449077129364),\n",
       " ('eldest', 0.7184861898422241),\n",
       " ('widow', 0.7099431157112122)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analogy example\n",
    "w2v_model.most_similar(\n",
    "    positive=[\"woman\", \"king\"],\n",
    "    negative=[\"man\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71b3ee52-eb07-498b-b73c-2e8cacd4ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# predefined questions\n",
    "questions = [\n",
    "    \"what is your name\",\n",
    "    \"how are you\",\n",
    "    \"what is machine learning\",\n",
    "    \"what is python\",\n",
    "    \"what is nlp\",\n",
    "    \"who created python\",\n",
    "    \"what is data science\",\n",
    "    \"what is artificial intelligence\"\n",
    "]\n",
    "\n",
    "# answers\n",
    "answers = [\n",
    "    \"I am an NLP internship chatbot.\",\n",
    "    \"I am doing great!\",\n",
    "    \"Machine learning is a field where computers learn from data.\",\n",
    "    \"Python is a programming language used in AI.\",\n",
    "    \"NLP means Natural Language Processing.\",\n",
    "    \"Python was created by Guido van Rossum.\",\n",
    "    \"Data science is extracting insights from data.\",\n",
    "    \"Artificial Intelligence means machines that think like humans.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a18bcea-a771-4234-8815-983045965f39",
   "metadata": {},
   "source": [
    "# convert questions into vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "Q_vec = vectorizer.fit_transform(questions)\n",
    "\n",
    "# chatbot function\n",
    "def chatbot(user_input):\n",
    "    user_vec = vectorizer.transform([user_input])\n",
    "    sim = cosine_similarity(user_vec, Q_vec)\n",
    "    idx = sim.argmax()\n",
    "    return answers[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed40251f-cb35-4cf8-99ae-af49152c8d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a programming language used in AI.\n",
      "Python is a programming language used in AI.\n",
      "I am doing great!\n"
     ]
    }
   ],
   "source": [
    "print(chatbot(\"tell me about python\"))\n",
    "print(chatbot(\"what is ai\"))\n",
    "print(chatbot(\"how are you\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "981fb511-dfbf-4e68-a74a-2a73c8dab249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 – NLP Internship Assignment\n",
    "## Text Classification, Word Embeddings & FAQ Chatbot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360be7cb-edbb-48d6-94a7-9b8bfaa6541b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca13e15a-8af3-497d-aa67-263b40fe525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 – Text Classification Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f1e969a-990f-4b66-b3d1-9ec32899aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 – Word Embedding Mini Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34145ca2-7b87-4fd3-8ad5-c076ea6d3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 – Mini NLP Application – FAQ Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b26e59-2255-406c-9813-1fe4879931da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_env]",
   "language": "python",
   "name": "conda-env-nlp_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
