{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597e7f2b-7608-4f39-b370-ad38a2fe37ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>basically there is a family where a little boy...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>petter mattei s love in the time of money is a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "\n",
       "                                      cleaned_review sentiment  \n",
       "0  one of the other reviewers has mentioned that ...  positive  \n",
       "1  a wonderful little production the filming tech...  positive  \n",
       "2  i thought this was a wonderful way to spend ti...  positive  \n",
       "3  basically there is a family where a little boy...  negative  \n",
       "4  petter mattei s love in the time of money is a...  positive  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"IMDB_cleaned.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee35826f-ee91-4125-81c3-a7bc45faef8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. The ...\n",
      "VADER Scores: {'neg': 0.203, 'neu': 0.748, 'pos': 0.048, 'compound': -0.9951}\n",
      "------------------------------------------------------------\n",
      "Review: A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-B ...\n",
      "VADER Scores: {'neg': 0.053, 'neu': 0.776, 'pos': 0.172, 'compound': 0.9641}\n",
      "------------------------------------------------------------\n",
      "Review: I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air con ...\n",
      "VADER Scores: {'neg': 0.094, 'neu': 0.714, 'pos': 0.192, 'compound': 0.9605}\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Take 3 sample reviews\n",
    "sample_reviews = df['review'].head(3)\n",
    "\n",
    "for review in sample_reviews:\n",
    "    scores = sia.polarity_scores(review)\n",
    "    print(\"Review:\", review[:100], \"...\")\n",
    "    print(\"VADER Scores:\", scores)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed62b33-384c-4b87-a119-e5e90dfae21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Review:\n",
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me abo\n",
      "\n",
      "Number of Sentences: 10\n",
      "Number of Tokens: 380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "review_text = df['review'][0]\n",
    "\n",
    "sentences = sent_tokenize(review_text)\n",
    "tokens = word_tokenize(review_text)\n",
    "\n",
    "print(\"Sample Review:\")\n",
    "print(review_text[:200])\n",
    "\n",
    "print(\"\\nNumber of Sentences:\", len(sentences))\n",
    "print(\"Number of Tokens:\", len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f410e68c-ccc1-4c75-94f8-3fa9e18cad0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', 'oz', 'episode', 'you', 'will', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first']\n",
      "After: ['one', 'reviewers', 'mentioned', 'watching', 'oz', 'episode', 'hooked', 'right', 'exactly', 'happened', 'first', 'thing', 'struck', 'oz', 'brutality', 'unflinching', 'scenes', 'violence', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'hearted', 'timid', 'show', 'pulls', 'punches']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = df['cleaned_review'][0]\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [w for w in tokens if w.lower() not in stop_words]\n",
    "\n",
    "print(\"Before:\", tokens[:30])\n",
    "print(\"After:\", filtered_tokens[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5aae37-2128-4387-93dd-984252fe0463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one → one | one\n",
      "reviewers → review | reviewer\n",
      "mentioned → mention | mentioned\n",
      "watching → watch | watching\n",
      "oz → oz | oz\n",
      "episode → episod | episode\n",
      "hooked → hook | hooked\n",
      "right → right | right\n",
      "exactly → exactli | exactly\n",
      "happened → happen | happened\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = filtered_tokens[:10]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"→\", stemmer.stem(word), \"|\", lemmatizer.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4ac0d85-b73f-4b45-b0ea-067c01164e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked. they are right, as this is exactly what happened with me.<br /><br />the first thing that struck me abo\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = df['review'][0]\n",
    "\n",
    "normalized_text = re.sub(r'\\s+', ' ', text.strip().lower())\n",
    "\n",
    "print(normalized_text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae91e6e7-c775-41b0-8039-7065476055eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just  Oz episode youll be hooked They are right as this is exactly what happened with mebr br The first thing that struck me about Oz was i\n"
     ]
    }
   ],
   "source": [
    "text = df['review'][0]\n",
    "\n",
    "clean_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "print(clean_text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f2ecec-d79e-4324-a0dc-1e52dd7e0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = df['cleaned_review'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "019065ba-aac1-4d7c-a0e9-35c9a79964aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vocabulary:\n",
      "['all' 'and' 'are' 'as' 'be' 'but' 'for' 'in' 'is' 'it' 'not' 'of' 'one'\n",
      " 'that' 'the' 'this' 'to' 'with' 'would' 'you']\n",
      "\n",
      "BoW Matrix:\n",
      "[[ 1  6  2  4  2  2  5  3 10  6  6  7  1  5 16  3  6  5  2  3]\n",
      " [ 2  7  2  0  0  2  0  0  3  3  2  5  1  0 16  0  2  3  0  1]\n",
      " [ 0  4  1  0  2  3  0  4  4  2  2  4  1  1  8  5  4  2  1  0]\n",
      " [ 3  4  3  2  0  0  2  2  4  0  0  2  0  0  6  1  3  3  0  2]\n",
      " [ 2  5  1  1  1  1  1  6  7  0  1  6  6  1 20  2  7  1  0  0]\n",
      " [ 2  5  1  1  1  2  1  3  4  3  1  2  1  1  4  1  4  0  1  0]\n",
      " [ 0  5  0  0  3  0  3  3  1  2  0  4  0  0  4  3  6  2  6  3]\n",
      " [ 0  2  1  2  1  1  2  1  7  8  3  4  1  5 15  4  3  1  1  0]\n",
      " [ 0  3  0  0  1  0  1  2  6  1  0  4  2  1  7  5  3  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  1  0  0  0  0  0  2  0  0  0  4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow = CountVectorizer(max_features=20)\n",
    "bow_matrix = bow.fit_transform(sample_data)\n",
    "\n",
    "print(\"BoW Vocabulary:\")\n",
    "print(bow.get_feature_names_out())\n",
    "\n",
    "print(\"\\nBoW Matrix:\")\n",
    "print(bow_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ba8935b-966f-4e89-876f-4055912fde8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vocabulary:\n",
      "['all' 'and' 'are' 'as' 'be' 'but' 'for' 'in' 'is' 'it' 'not' 'of' 'one'\n",
      " 'that' 'the' 'this' 'to' 'with' 'would' 'you']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.0517374  0.21169505 0.07735283 0.20694959 0.08494097 0.09354369\n",
      "  0.21235242 0.11602925 0.35282508 0.2320585  0.28063106 0.24697755\n",
      "  0.04247048 0.23385922 0.56452012 0.10584752 0.21169505 0.21235242\n",
      "  0.1034748  0.15521219]\n",
      " [0.14702016 0.35091326 0.10990527 0.         0.         0.13290973\n",
      "  0.         0.         0.1503914  0.16485791 0.13290973 0.25065233\n",
      "  0.06034336 0.         0.80208746 0.         0.10026093 0.18103009\n",
      "  0.         0.07351008]\n",
      " [0.         0.27089697 0.0742388  0.         0.16304291 0.26933355\n",
      "  0.         0.2969552  0.27089697 0.1484776  0.1795557  0.27089697\n",
      "  0.08152146 0.08977785 0.54179395 0.33862122 0.27089697 0.16304291\n",
      "  0.09930916 0.        ]\n",
      " [0.3556342  0.32336805 0.2658552  0.23708946 0.         0.\n",
      "  0.19462332 0.1772368  0.32336805 0.         0.         0.16168403\n",
      "  0.         0.         0.48505208 0.08084201 0.24252604 0.29193498\n",
      "  0.         0.23708946]\n",
      " [0.11246642 0.19174221 0.04203727 0.05623321 0.04616103 0.05083616\n",
      "  0.04616103 0.25222363 0.2684391  0.         0.05083616 0.23009065\n",
      "  0.27696618 0.05083616 0.76696885 0.07669688 0.2684391  0.04616103\n",
      "  0.         0.        ]\n",
      " [0.258592   0.4408694  0.09665554 0.129296   0.10613722 0.23377335\n",
      "  0.10613722 0.28996661 0.35269552 0.28996661 0.11688667 0.17634776\n",
      "  0.10613722 0.11688667 0.35269552 0.08817388 0.35269552 0.\n",
      "  0.129296   0.        ]\n",
      " [0.         0.31797514 0.         0.         0.22965303 0.\n",
      "  0.22965303 0.20913716 0.06359503 0.13942477 0.         0.25438011\n",
      "  0.         0.         0.25438011 0.19078508 0.38157017 0.15310202\n",
      "  0.55952507 0.27976254]\n",
      " [0.         0.09121983 0.04999724 0.1337625  0.05490186 0.06046225\n",
      "  0.10980371 0.04999724 0.3192694  0.39997793 0.18138674 0.18243965\n",
      "  0.05490186 0.30231124 0.6841487  0.18243965 0.13682974 0.05490186\n",
      "  0.06688125 0.        ]\n",
      " [0.         0.23683118 0.         0.         0.09502664 0.\n",
      "  0.09502664 0.17307503 0.47366236 0.08653751 0.         0.31577491\n",
      "  0.19005329 0.10465083 0.55260609 0.39471863 0.23683118 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.17160006 0.         0.         0.\n",
      "  0.         0.         0.         0.17160006 0.         0.\n",
      "  0.         0.         0.         0.31308383 0.         0.\n",
      "  0.         0.9181968 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=20)\n",
    "tfidf_matrix = tfidf.fit_transform(sample_data)\n",
    "\n",
    "print(\"TF-IDF Vocabulary:\")\n",
    "print(tfidf.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
