{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ef281a",
   "metadata": {},
   "source": [
    "csv file :- https://www.kaggle.com/datasets/yasserh/amazon-product-reviews-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb8c99",
   "metadata": {},
   "source": [
    "Task 1: Write a short Python script using any open dataset (e.g., movie reviews, tweets) to demonstrate one real-world NLP\n",
    "application:\n",
    "• Sentiment analysis (positive/negative)\n",
    "• Text summarization (using a library like sumy or gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30930528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0: I initially had trouble deciding between the paperwhite and the voyage because reviews more or less ...\n",
      "Sentiment: Positive\n",
      "\n",
      "Review 1: Allow me to preface this with a little history. I am (was) a casual reader who owned a Nook Simple T...\n",
      "Sentiment: Positive\n",
      "\n",
      "Review 2: I am enjoying it so far. Great for reading. Had the original Fire since 2012. The Fire used to make ...\n",
      "Sentiment: Positive\n",
      "\n",
      "Review 3: I bought one of the first Paperwhites and have been very pleased with it its been a constant compani...\n",
      "Sentiment: Positive\n",
      "\n",
      "Review 4: I have to say upfront - I don't like coroporate, hermetically closed stuff like anything by Apple or...\n",
      "Sentiment: Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv('7817_1.csv')\n",
    "positive_words = ['good', 'great', 'excellent', 'love', 'amazing', 'fantastic']\n",
    "negative_words = ['bad', 'terrible', 'hate', 'boring', 'awful', 'disappointing']\n",
    "def sentiment_analysis(text):\n",
    "    if pd.isna(text):\n",
    "        return \"Neutral\"\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    pos_count = sum(1 for word in words if word in positive_words)\n",
    "    neg_count = sum(1 for word in words if word in negative_words)\n",
    "    if pos_count > neg_count:\n",
    "        return \"Positive\"\n",
    "    elif neg_count > pos_count:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "# Apply to first 5 reviews\n",
    "for i, row in df.head(5).iterrows():\n",
    "    print(f\"Review {i}: {row['reviews.text'][:100]}...\")\n",
    "    print(f\"Sentiment: {sentiment_analysis(row['reviews.text'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313d4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (first 200 chars): I initially had trouble deciding between the paperwhite and the voyage because reviews more or less said the same thing: the paperwhite is great, but if you have spending money, go for the voyage.Fort...\n",
      "Summary: I initially had trouble deciding between the paperwhite and the voyage because reviews more or less said the same thing: the paperwhite is great, but if you have spending money, go for the voyage.Fortunately, I had friends who owned each, so I ended up buying the paperwhite on this basis: both models now have 300 ppi, so the 80 dollar jump turns out pricey the voyage's page press isn't always sensitive, and if you are fine with a specific setting, you don't need auto light adjustment).It's been a week and I am loving my paperwhite, no regrets! The touch screen is receptive and easy to use, and I keep the light at a specific setting regardless of the time of day. (In any case, it's not hard to change the setting either, as you'll only be changing the light level at a certain time of day, not every now and then while reading).Also glad that I went for the international shipping option with Amazon. Extra expense, but delivery was on time, with tracking, and I didnt need to worry about customs, which I may have if I used a third party shipping service.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv('7817_1.csv')\n",
    "def text_summarization(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return ' '.join(sentences[:3])\n",
    "# Apply to first review\n",
    "text = df['reviews.text'].iloc[0]\n",
    "print(\"Original (first 200 chars):\", text[:200] + \"...\")\n",
    "print(\"Summary:\", text_summarization(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911911d",
   "metadata": {},
   "source": [
    "Text Data Basics\n",
    "Task 2: Write a Python function that takes a paragraph and outputs:\n",
    "• List of sentences\n",
    "• List of tokens (words)\n",
    "• Count of tokens, sentences, paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "965235d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: [\"I initially had trouble deciding between the paperwhite and the voyage because reviews more or less said the same thing: the paperwhite is great, but if you have spending money, go for the voyage.Fortunately, I had friends who owned each, so I ended up buying the paperwhite on this basis: both models now have 300 ppi, so the 80 dollar jump turns out pricey the voyage's page press isn't always sensitive, and if you are fine with a specific setting, you don't need auto light adjustment).It's been a week and I am loving my paperwhite, no regrets! The touch screen is receptive and easy to use, and I keep the light at a specific setting regardless of the time of day.\", \"(In any case, it's not hard to change the setting either, as you'll only be changing the light level at a certain time of day, not every now and then while reading).Also glad that I went for the international shipping option with Amazon.\", 'Extra expense, but delivery was on time, with tracking, and I didnt need to worry about customs, which I may have if I used a third party shipping service.'] ...\n",
      "Tokens: ['I', 'initially', 'had', 'trouble', 'deciding', 'between', 'the', 'paperwhite', 'and', 'the'] ...\n",
      "Counts: {'tokens': 202, 'sentences': 3, 'paragraphs': 1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "def process_paragraph(paragraph):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', paragraph)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    tokens = re.findall(r'\\b\\w+\\b', paragraph)\n",
    "    num_sentences = len(sentences)\n",
    "    num_tokens = len(tokens)\n",
    "    num_paragraphs = 1\n",
    "    return {\n",
    "        'sentences': sentences,\n",
    "        'tokens': tokens,\n",
    "        'counts': {\n",
    "            'tokens': num_tokens,\n",
    "            'sentences': num_sentences,\n",
    "            'paragraphs': num_paragraphs\n",
    "        }\n",
    "    }\n",
    "df = pd.read_csv('7817_1.csv')\n",
    "paragraph = df['reviews.text'].iloc[0]\n",
    "result = process_paragraph(paragraph)\n",
    "print(\"Sentences:\", result['sentences'][:3], \"...\")\n",
    "print(\"Tokens:\", result['tokens'][:10], \"...\")\n",
    "print(\"Counts:\", result['counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d274a9",
   "metadata": {},
   "source": [
    "Text Preprocessing\n",
    "A. Tokenization Task: Tokenize a text document using simple split vs. regex tokenizer.\n",
    "Compare outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80109ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I initially had trouble deciding between the paperwhite and the voyage because reviews more or less said the same thing: the paperwhite is great, but if you have spending money, go for the voyage.Fort\n",
      "Split tokens: ['I', 'initially', 'had', 'trouble', 'deciding', 'between', 'the', 'paperwhite', 'and', 'the', 'voyage', 'because', 'reviews', 'more', 'or', 'less', 'said', 'the', 'same', 'thing:', 'the', 'paperwhite', 'is', 'great,', 'but', 'if', 'you', 'have', 'spending', 'money,', 'go', 'for', 'the', 'voyage.Fort']\n",
      "Regex tokens: ['I', 'initially', 'had', 'trouble', 'deciding', 'between', 'the', 'paperwhite', 'and', 'the', 'voyage', 'because', 'reviews', 'more', 'or', 'less', 'said', 'the', 'same', 'thing', 'the', 'paperwhite', 'is', 'great', 'but', 'if', 'you', 'have', 'spending', 'money', 'go', 'for', 'the', 'voyage', 'Fort']\n",
      "Difference: Split includes punctuation attached, regex removes non-word chars.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv('7817_1.csv')\n",
    "text = df['reviews.text'].iloc[0][:200]\n",
    "print(\"Text:\", text)\n",
    "tokens_split = text.split()\n",
    "tokens_re = re.findall(r'\\b\\w+\\b', text)\n",
    "print(\"Split tokens:\", tokens_split)\n",
    "print(\"Regex tokens:\", tokens_re)\n",
    "print(\"Difference: Split includes punctuation attached, regex removes non-word chars.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98abe003",
   "metadata": {},
   "source": [
    "B. Stopwords Removal\n",
    "Task: Remove stopwords from a text and print before vs after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ebec18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: I initially had trouble deciding between the paperwhite and the voyage because reviews more or less said the same thing: the paperwhite is great, but if you have spending money, go for the voyage.Fort\n",
      "After: initially had trouble deciding between paperwhite voyage because reviews more or less said same thing paperwhite great but if you have spending money go voyage fort\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv('7817_1.csv')\n",
    "stopwords = ['the', 'is', 'a', 'an', 'and', 'of', 'to', 'in', 'for', 'on', 'i', 'my', 'with']\n",
    "def remove_stopwords(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    filtered = [word for word in tokens if word not in stopwords]\n",
    "    return ' '.join(filtered)\n",
    "text = df['reviews.text'].iloc[0][:200]\n",
    "print(\"Before:\", text)\n",
    "print(\"After:\", remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f8b00",
   "metadata": {},
   "source": [
    "C.Lemmatization & Stemming\n",
    "Task: Apply simple rule-based stemmer and lemmatizer on a text. Show differences\n",
    "(e.g., “studies” → “studi” vs. “study”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "491d1cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['He', 'studies', 'hard', 'and', 'is', 'studying', 'now', 'Studied', 'yesterday']\n",
      "Stemmed: ['he', 'study', 'hard', 'and', 'i', 'study', 'now', 'studi', 'yesterday']\n",
      "Lemmatized: ['he', 'study', 'hard', 'and', 'is', 'studying', 'now', 'studied', 'yesterday']\n",
      "Difference example: “studies” → “studi” (stem) vs. “study” (lemma)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def simple_stem(word):\n",
    "    word = word.lower()\n",
    "    if word.endswith('ies'):\n",
    "        return word[:-3] + 'y'\n",
    "    elif word.endswith('ing'):\n",
    "        return word[:-3]\n",
    "    elif word.endswith('ed'):\n",
    "        return word[:-2]\n",
    "    elif word.endswith('es'):\n",
    "        return word[:-2]\n",
    "    elif word.endswith('s'):\n",
    "        return word[:-1]\n",
    "    return word\n",
    "def simple_lemma(word):\n",
    "    # Very basic, e.g. studies -> study\n",
    "    word = word.lower()\n",
    "    if word == 'studies':\n",
    "        return 'study'\n",
    "    if word == 'studi':\n",
    "        return 'study'  # after stem\n",
    "    return word\n",
    "text = \"He studies hard and is studying now. Studied yesterday.\"\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "stemmed = [simple_stem(word) for word in tokens]\n",
    "lemmatized = [simple_lemma(word) for word in tokens]\n",
    "print(\"Original:\", tokens)\n",
    "print(\"Stemmed:\", stemmed)\n",
    "print(\"Lemmatized:\", lemmatized)\n",
    "print('Difference example: “studies” → “studi” (stem) vs. “study” (lemma)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabfd97a",
   "metadata": {},
   "source": [
    "D.Handling punctuation, special characters, emojis\n",
    "Task: Clean text containing #hashtags, @mentions, !!!, �� using regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d33bd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Hello @user! #cool ��This is great!!!\n",
      "After: Hello   This is great\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'#\\w+', '', text)  # Hashtags\n",
    "    text = re.sub(r'@\\w+', '', text)  # Mentions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Punctuation and special chars\n",
    "    text = re.sub(r'[\\U0001F600-\\U0001F6FF\\U0001F300-\\U0001F5FF]', '', text)  # Emojis\n",
    "    return text.strip()\n",
    "text = \"Hello @user! #cool ��This is great!!!\"\n",
    "print(\"Before:\", text)\n",
    "print(\"After:\", clean_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9584f8f4",
   "metadata": {},
   "source": [
    "E. Lowercasing & Normalization\n",
    "Task: Convert mixed-case text into lowercase and normalize spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5b078b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: This   IS Mixed Case   Text With   Extra Spaces.\n",
      "After: this is mixed case text with extra spaces.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize multiple spaces\n",
    "    return text.strip()\n",
    "text = \"This   IS Mixed Case   Text With   Extra Spaces.\"\n",
    "print(\"Before:\", text)\n",
    "print(\"After:\", normalize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e4800",
   "metadata": {},
   "source": [
    "F. Regex for text cleaning\n",
    "Task: Extract all emails from a paragraph. Remove numbers from text. Replace multiple spaces with one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eda286c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails: ['example@email.com', 'test123@example.com']\n",
      "Without numbers: Contact me at example@email.com or test@example.com. The year is .   Extra   spaces.\n",
      "Normalized spaces: Contact me at example@email.com or test@example.com. The year is . Extra spaces.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_emails(text):\n",
    "    return re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def replace_multiple_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "paragraph = \"Contact me at example@email.com or test123@example.com. The year is 2023.   Extra   spaces.\"\n",
    "print(\"Emails:\", extract_emails(paragraph))\n",
    "cleaned = remove_numbers(paragraph)\n",
    "print(\"Without numbers:\", cleaned)\n",
    "print(\"Normalized spaces:\", replace_multiple_spaces(cleaned))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
