{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8385f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups(\n",
    "    remove=('headers', 'footers', 'quotes')\n",
    ")\n",
    "documents = data.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbdb3cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    tokens = [w for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "clean_docs = [clean_text(doc) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3328809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=10)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=10)\n",
    "\n",
    "X_count = count_vectorizer.fit_transform(clean_docs)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(clean_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d80fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10,\n",
    "    random_state=42,\n",
    "    learning_method='batch'\n",
    ")\n",
    "\n",
    "lda_topics = lda.fit_transform(X_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d07b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(\n",
    "    n_components=10,\n",
    "    random_state=42,\n",
    "    init='nndsvd'\n",
    ")\n",
    "\n",
    "nmf_topics = nmf.fit_transform(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f7f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import numpy as np\n",
    "\n",
    "# Topic-word distribution\n",
    "topic_term_dists = lda.components_\n",
    "topic_term_dists = topic_term_dists / topic_term_dists.sum(axis=1)[:, None]\n",
    "\n",
    "# Document-topic distribution\n",
    "doc_topic_dists = lda.transform(X_count)\n",
    "\n",
    "# Document lengths\n",
    "doc_lengths = np.array(X_count.sum(axis=1)).flatten()\n",
    "\n",
    "# Vocabulary\n",
    "vocab = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Term frequencies\n",
    "term_frequency = np.array(X_count.sum(axis=0)).flatten()\n",
    "\n",
    "\n",
    "lda_vis = pyLDAvis.prepare(\n",
    "    topic_term_dists=topic_term_dists,\n",
    "    doc_topic_dists=doc_topic_dists,\n",
    "    doc_lengths=doc_lengths,\n",
    "    vocab=vocab,\n",
    "    term_frequency=term_frequency\n",
    ")\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_vis\n",
    "\n",
    "pyLDAvis.save_html(lda_vis, \"lda.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de563157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "897772eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š TOPIC MODEL EVALUATION (NO gensim)\n",
      "\n",
      "LDA:\n",
      "  PMI Coherence   : 1.6853\n",
      "  Topic Diversity : 0.7700\n",
      "  Perplexity      : 2546.13\n",
      "\n",
      "NMF:\n",
      "  PMI Coherence   : 1.9921\n",
      "  Topic Diversity : 0.9500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_top_words(model, feature_names, top_n=10):\n",
    "    topics = []\n",
    "    for topic in model.components_:\n",
    "        top_ids = topic.argsort()[-top_n:]\n",
    "        topics.append([feature_names[i] for i in top_ids])\n",
    "    return topics\n",
    "\n",
    "\n",
    "def pmi_coherence(topics, X, vocab):\n",
    "    \"\"\"\n",
    "    PMI-based topic coherence\n",
    "    Works with sparse CSR matrices\n",
    "    \"\"\"\n",
    "    word2id = {w: i for i, w in enumerate(vocab)}\n",
    "    X_bin = (X > 0).astype(np.int8)   # keep sparse\n",
    "\n",
    "    topic_scores = []\n",
    "\n",
    "    for topic in topics:\n",
    "        ids = [word2id[w] for w in topic if w in word2id]\n",
    "        score = 0\n",
    "        pairs = 0\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "            wi = ids[i]\n",
    "            col_i = X_bin[:, wi].toarray().ravel()\n",
    "\n",
    "            p_i = col_i.mean()\n",
    "\n",
    "            for j in range(i + 1, len(ids)):\n",
    "                wj = ids[j]\n",
    "                col_j = X_bin[:, wj].toarray().ravel()\n",
    "\n",
    "                p_j = col_j.mean()\n",
    "                p_ij = (col_i & col_j).mean()\n",
    "\n",
    "                if p_ij > 0:\n",
    "                    score += np.log(p_ij / (p_i * p_j))\n",
    "                    pairs += 1\n",
    "\n",
    "        if pairs > 0:\n",
    "            topic_scores.append(score / pairs)\n",
    "\n",
    "    return float(np.mean(topic_scores))\n",
    "\n",
    "\n",
    "def topic_diversity(topics):\n",
    "    unique_words = set()\n",
    "    total_words = 0\n",
    "    for topic in topics:\n",
    "        unique_words.update(topic)\n",
    "        total_words += len(topic)\n",
    "    return len(unique_words) / total_words\n",
    "\n",
    "\n",
    "\n",
    "lda_topics_words = extract_top_words(\n",
    "    lda,\n",
    "    count_vectorizer.get_feature_names_out(),\n",
    "    top_n=10\n",
    ")\n",
    "\n",
    "nmf_topics_words = extract_top_words(\n",
    "    nmf,\n",
    "    tfidf_vectorizer.get_feature_names_out(),\n",
    "    top_n=10\n",
    ")\n",
    "\n",
    "\n",
    "lda_pmi = pmi_coherence(\n",
    "    lda_topics_words,\n",
    "    X_count,\n",
    "    count_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "nmf_pmi = pmi_coherence(\n",
    "    nmf_topics_words,\n",
    "    X_count,\n",
    "    count_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "lda_diversity = topic_diversity(lda_topics_words)\n",
    "nmf_diversity = topic_diversity(nmf_topics_words)\n",
    "\n",
    "lda_perplexity = lda.perplexity(X_count)\n",
    "\n",
    "\n",
    "print(\"ðŸ“Š TOPIC MODEL EVALUATION (NO gensim)\\n\")\n",
    "\n",
    "print(\"LDA:\")\n",
    "print(f\"  PMI Coherence   : {lda_pmi:.4f}\")\n",
    "print(f\"  Topic Diversity : {lda_diversity:.4f}\")\n",
    "print(f\"  Perplexity      : {lda_perplexity:.2f}\\n\")\n",
    "\n",
    "print(\"NMF:\")\n",
    "print(f\"  PMI Coherence   : {nmf_pmi:.4f}\")\n",
    "print(f\"  Topic Diversity : {nmf_diversity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93638688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashis\\OneDrive\\Desktop\\Ashish Khedkar\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 240.14it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "2026-02-18 11:59:15,939 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 354/354 [06:27<00:00,  1.09s/it]\n",
      "2026-02-18 12:05:44,814 - BERTopic - Embedding - Completed âœ“\n",
      "2026-02-18 12:05:44,817 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-02-18 12:06:33,474 - BERTopic - Dimensionality - Completed âœ“\n",
      "2026-02-18 12:06:33,479 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-02-18 12:06:34,164 - BERTopic - Cluster - Completed âœ“\n",
      "2026-02-18 12:06:34,179 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-02-18 12:06:41,577 - BERTopic - Representation - Completed âœ“\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                           Name  \\\n",
      "0     -1   3400          -1_would_one_edu_like   \n",
      "1      0   1074        0_team_game_season_year   \n",
      "2      1    940            1_car_bike_cars_dod   \n",
      "3      2    842          2_god_jesus_one_bible   \n",
      "4      3    518  3_health_medical_patients_msg   \n",
      "5      4    421      4_space_nasa_launch_orbit   \n",
      "6      5    405  5_key_encryption_chip_clipper   \n",
      "7      6    265     6_israel_israeli_jews_arab   \n",
      "8      7    264       7_card_monitor_video_vga   \n",
      "9      8    184       8_scsi_drive_disk_drives   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [would, one, edu, like, know, get, use, people...   \n",
      "1  [team, game, season, year, play, games, hockey...   \n",
      "2  [car, bike, cars, dod, engine, like, one, get,...   \n",
      "3  [god, jesus, one, bible, believe, people, fait...   \n",
      "4  [health, medical, patients, msg, disease, food...   \n",
      "5  [space, nasa, launch, orbit, lunar, earth, sat...   \n",
      "6  [key, encryption, chip, clipper, privacy, entr...   \n",
      "7  [israel, israeli, jews, arab, arabs, palestini...   \n",
      "8  [card, monitor, video, vga, drivers, cards, wi...   \n",
      "9  [scsi, drive, disk, drives, hard, controller, ...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [issue never whether tanks used detroit 1967 w...  \n",
      "1  [philadelphia buffalo first period philadelphi...  \n",
      "2  [howdy little new newsgroup would like tap kno...  \n",
      "3  [listmember andrew killie think wrote response...  \n",
      "4  [cut university arizona tucson arizona suggest...  \n",
      "5  [archive name space probe last modified date p...  \n",
      "6  [document anonymous ftp directory nist looks l...  \n",
      "7  [center policy research cpr subject final solu...  \n",
      "8  [anyone know makes cable adapter wired accordi...  \n",
      "9  [complain maybe scsi expensive year two ago is...  \n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Use a light, fast transformer\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    n_gram_range=(1, 2),\n",
    "    min_topic_size=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "topics, probs = topic_model.fit_transform(clean_docs)\n",
    "\n",
    "# Show topic info\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66e46cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('car', 0.01700741107839629),\n",
       " ('bike', 0.013116913636620707),\n",
       " ('cars', 0.008516138913604806),\n",
       " ('dod', 0.007549254834004425),\n",
       " ('engine', 0.007322223222280939),\n",
       " ('like', 0.006502844766136577),\n",
       " ('one', 0.006335674991584787),\n",
       " ('get', 0.005832513503028948),\n",
       " ('would', 0.005747821907652425),\n",
       " ('good', 0.005715622324327552)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf2d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.get_document_info(clean_docs[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2056af49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic Diversity: 0.7918032786885246\n"
     ]
    }
   ],
   "source": [
    "len(set(topics)) - (1 if -1 in topics else 0)\n",
    "bertopic_topics = [\n",
    "    [word for word, _ in topic_model.get_topic(t)]\n",
    "    for t in topic_model.get_topics()\n",
    "    if t != -1\n",
    "]\n",
    "\n",
    "bertopic_diversity = topic_diversity(bertopic_topics)\n",
    "print(\"BERTopic Diversity:\", bertopic_diversity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fdf41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = topic_model.visualize_topics()\n",
    "fig.write_html(\"bertopic_topics.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1854301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = topic_model.visualize_hierarchy()\n",
    "fig.write_html(\"bertopic_hierarchy.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0b53bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = topic_model.visualize_barchart()\n",
    "fig.write_html(\"bertopic_barchart.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433083b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_info = topic_model.get_document_info(clean_docs)\n",
    "# doc_info.to_csv(\"bertopic_document_topics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8875a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
